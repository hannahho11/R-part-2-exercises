---
title: "author_attrib"
author: "hannah"
date: "8/18/2019"
output: pdf_document
---
```{r setup, include=FALSE, echo=FALSE}
require("knitr")
opts_knit$set(root.dir = "C:/Users/Hannah/Desktop/UT Austin/Analytics - MSBA/summer 19/predictive modeling/STA380-1-master/STA380-1-master/R")
```

```{r}
library(tm)
library(magrittr)

# Remember to source in the "reader" wrapper function
# it's stored as a Github gist at:
# https://gist.github.com/jgscott/28d9d1287a0c3c1477e2113f6758d5ff

readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }

## Rolling two directories together into a single corpus
author_dirs = Sys.glob('../data/ReutersC50/C50train/*')
file_list = NULL
labels = NULL
for(author in author_dirs) {
  author_name = substring(author, first=29) #author name starts at 29th char
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  labels = append(labels, rep(author_name, length(files_to_add)))
}

# Need a more clever regex to get better names here
all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))

# Clean up the file names, append author to filename
# This uses the piping operator from magrittr
# See https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html
mynames = file_list %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

names(all_docs) = mynames 
my_corpus = Corpus(VectorSource(all_docs))

# Preprocessing
my_corpus = tm_map(my_corpus, content_transformer(tolower)) # make everything lowercase
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))

DTM = DocumentTermMatrix(my_corpus)
DTM # some basic summary statistics
```


```{r}
#########################################################
#TF-IDF
## Removes those terms that have count 0 in >95% of docs.  
DTM = removeSparseTerms(DTM, 0.975) #0.95 arbitrary choice.
DTM # now ~ 1411 terms

# construct TF IDF weights
tfidf = weightTfIdf(DTM)
```

#Preprocess test data
```{r}
author_dirs_test = Sys.glob('../data/ReutersC50/C50test/*')
file_list_test = NULL
labels_test = NULL
for(author in author_dirs_test) {
  author_name = substring(author, first=28) #author name starts at 29th char
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_test = append(file_list_test, files_to_add)
  labels_test = append(labels_test, rep(author_name, length(files_to_add)))
}

# Need a more clever regex to get better names here
all_docs_test = lapply(file_list_test, readerPlain) 
names(all_docs_test) = file_list_test
names(all_docs_test) = sub('.txt', '', names(all_docs_test))

# Clean up the file names, append author to filename
# This uses the piping operator from magrittr
# See https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html
mynames_test = file_list %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

names(all_docs_test) = mynames_test 
my_corpus_test = Corpus(VectorSource(all_docs_test))

# Preprocessing test data
my_corpus_test = tm_map(my_corpus_test, content_transformer(tolower)) # make everything lowercase
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeNumbers)) # remove numbers
my_corpus_test = tm_map(my_corpus_test, content_transformer(removePunctuation)) # remove punctuation
my_corpus_test = tm_map(my_corpus_test, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeWords), stopwords("SMART"))

# IGNORE words not seen before
DTM_test = DocumentTermMatrix(my_corpus_test,control=list(dictionary=Terms(DTM)))
summary(Terms(DTM_test) %in% Terms(DTM)) # some basic summary statistics
```

```{r}
#TF-IDF on test
# construct TF IDF weights
tfidf_test = weightTfIdf(DTM_test)
```
```{r}
# Now PCA on term frequencies for train data
X = as.matrix(tfidf)
summary(colSums(X))
scrub_cols = which(colSums(X) == 0)
X = X[,-scrub_cols]

pca = prcomp(X, scale=TRUE)
plot(pca)
#biplot(pca, scale = 0)
```
```{r}
# Look at the loadings
pca$rotation[order(abs(pca$rotation[,1]),decreasing=TRUE),1][1:25]
```

```{r}
#compute standard deviation of each principal component
std_dev <- pca$sdev

#compute variance
pc_var <- std_dev^2

#check variance of first 10 components
pc_var[1:10]
```


```{r}
#proportion of variance explained
prop_var <- pc_var/sum(pc_var)
prop_var[1:1]5
```
```{r}
#scree plot: line plot of the eigenvalues of principal components in an analysis, used to determine the number of principal components to keep in a PCA
plot(prop_var, xlab = "Principal Component",ylab = "Proportion of Variance Explained", type = "b")
```
```{r}
#cumulative scree plot
plot(cumsum(prop_var), xlab = "Principal Component",ylab = "Cumulative Proportion of Variance Explained",type = "b")
abline(v=600,lty=2,col=2,lwd=2) 
abline(h=0.8,lty=2,col=2,lwd=2) 
```
600 components explain a little over 80% of variance in the training set, so we will use the first 600 principal components. This is over a 50% decrease in dimension from the 1411 dimensions in the tfidf matrix. 
```{r}
#transform test into PCA
pca_test <- predict(pca, newdata = tfidf_test)
pca_test <- as.data.frame(pca_test)
```

```{r}
# #train knn analysis model on training set
# library(kknn)
# pr=kknn(labels~.,as.data.frame(pca$x[,1:600]),pca_test[,1:600],k=10, kernel = 'cos')
```
```{r}
library(class)

prediction = knn(pca$x[,1:200],pca_test[,1:200],cl=labels,k=3)
```
```{r}
library(readr)
prediction.table <- data.frame(ImageId=1:nrow(pca_test), Label=prediction)
write_csv(prediction.table, "tfidf_pca_200_knn_3.csv")
```
```{r}
#uses package caret(short for Classification And REgression Training)
library(caret)
result = confusionMatrix(table(prediction.table[,2], labels),dnn =c("Prediction", "Reference"),mode = "sens_spec")
overall_accuracy = result$overall['Accuracy']
overall_accuracy
```