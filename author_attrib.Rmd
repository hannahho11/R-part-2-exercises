---
title: "author_attrib"
author: "hannah ho"
date: "8/18/2019"
output: pdf_document
---
```{r setup, include=FALSE, echo=FALSE}
require("knitr")
opts_knit$set(root.dir = "C:/Users/Hannah/Desktop/UT Austin/Analytics - MSBA/summer 19/predictive modeling/STA380-1-master/STA380-1-master/R")
```

```{r}
library(tm)
library(magrittr)

# "reader" wrapper function
readerPlain = function(fname){
  readPlain(elem=list(content=readLines(fname)), 
            id=fname, language='en') }

# Rolling all directories into a single corpus
author_dirs = Sys.glob('../data/ReutersC50/C50train/*')
file_list = NULL
labels = NULL
for(author in author_dirs) {
  author_name = substring(author, first=29) #author name starts at 29th char
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list = append(file_list, files_to_add)
  labels = append(labels, rep(author_name, length(files_to_add)))
}

# Use regex to get better file names
all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))

# Clean up the file names, append author to filename
# Uses the piping operator from magrittr
# See https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html
mynames = file_list %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

names(all_docs) = mynames 
my_corpus = Corpus(VectorSource(all_docs))

# Preprocessing
my_corpus = tm_map(my_corpus, content_transformer(tolower)) # make everything lowercase
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("SMART"))

DTM = DocumentTermMatrix(my_corpus)
```

```{r}
# Create TF-IDF matrix
# Removes terms that have count 0 in >97.5% of docs.  
DTM = removeSparseTerms(DTM, 0.975) 
DTM # now ~ 1411 terms

# construct TF IDF weights
tfidf = weightTfIdf(DTM)
```

#Preprocess test data
```{r}
author_dirs_test = Sys.glob('../data/ReutersC50/C50test/*')
file_list_test = NULL
labels_test = NULL
for(author in author_dirs_test) {
  author_name = substring(author, first=28) #author name starts at 29th char
  files_to_add = Sys.glob(paste0(author, '/*.txt'))
  file_list_test = append(file_list_test, files_to_add)
  labels_test = append(labels_test, rep(author_name, length(files_to_add)))
}

# Need a more clever regex to get better names here
all_docs_test = lapply(file_list_test, readerPlain) 
names(all_docs_test) = file_list_test
names(all_docs_test) = sub('.txt', '', names(all_docs_test))

# Clean up the file names, append author to filename
# This uses the piping operator from magrittr
# See https://cran.r-project.org/web/packages/magrittr/vignettes/magrittr.html
mynames_test = file_list %>%
  { strsplit(., '/', fixed=TRUE) } %>%
  { lapply(., tail, n=2) } %>%
  { lapply(., paste0, collapse = '') } %>%
  unlist

names(all_docs_test) = mynames_test 
my_corpus_test = Corpus(VectorSource(all_docs_test))

# Preprocessing test data
my_corpus_test = tm_map(my_corpus_test, content_transformer(tolower)) # make everything lowercase
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeNumbers)) # remove numbers
my_corpus_test = tm_map(my_corpus_test, content_transformer(removePunctuation)) # remove punctuation
my_corpus_test = tm_map(my_corpus_test, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeWords), stopwords("SMART"))

# IGNORE words not seen before
DTM_test = DocumentTermMatrix(my_corpus_test,control=list(dictionary=Terms(DTM)))
summary(Terms(DTM_test) %in% Terms(DTM)) # some basic summary statistics
```

```{r}
# Create TF-IDF matrix of same dimensions on test data
tfidf_test = weightTfIdf(DTM_test)
```
```{r}
# Run PCA on term frequencies for train data
X = as.matrix(tfidf)
summary(colSums(X))
scrub_cols = which(colSums(X) == 0)
X = X[,-scrub_cols]

pca = prcomp(X, scale=TRUE)
plot(pca)
#biplot(pca, scale = 0) #another visualization, but too dense to be interpretable
```
```{r}
# Look at the loadings
pca$rotation[order(abs(pca$rotation[,1]),decreasing=TRUE),1][1:25]
```

```{r}
# Find optimal number of principal components to include in knn model
# Compute standard deviation of each principal component
std_dev <- pca$sdev

# Variance
pc_var <- std_dev^2

# Check variance of first 10 components
pc_var[1:10]
```
```{r}
# Proportion of variance explained
prop_var <- pc_var/sum(pc_var)
prop_var[1:15]
```
```{r}
# Scree plot: line plot of the eigenvalues of principal components in an analysis, used to determine the number of principal components to keep in a PCA
plot(prop_var, xlab = "Principal Component",ylab = "Proportion of Variance Explained", type = "b")
```
```{r}
# Cumulative scree plot as another visualization
plot(cumsum(prop_var), xlab = "Principal Component",ylab = "Cumulative Proportion of Variance Explained",type = "b")
abline(v=200,lty=2,col=2,lwd=2) 
abline(h=0.4,lty=2,col=2,lwd=2) 
```
600 components explain a little over 40% of variance in the training set, so we will use the first 200 principal components. Reducing the original 1411 dimensions in the tfidf matrix increases the effectiveness of knn analysis.
```{r}
# Transform test term frequency data into principal components.
pca_test <- predict(pca, newdata = tfidf_test)
pca_test <- as.data.frame(pca_test)
```
```{r}
# #train knn analysis model on training set
# library(kknn)
# pr=kknn(labels~.,as.data.frame(pca$x[,1:600]),pca_test[,1:600],k=10, kernel = 'cos')
```
```{r}
# Predict authors on test data using KNN model
library(class)

prediction = knn(pca$x[,1:200],pca_test[,1:200],cl=labels,k=3)
```
```{r}
# Write predictions to csv file
library(readr)
prediction.table <- data.frame(ImageId=1:nrow(pca_test), Label=prediction)
write_csv(prediction.table, "tfidf_pca_200_knn_3.csv")
```
```{r}
# Create confusion matrix and fetch accuracy rate of predictions
library(caret)
result = confusionMatrix(table(prediction.table[,2], labels),dnn =c("Prediction", "Reference"),mode = "sens_spec")
overall_accuracy = result$overall['Accuracy']
overall_accuracy
```
Running knn analysis with a value of k = 3 using the top 200 principal components derived from TF-IDF matrices on the test data results in a prediction accuracy of 43%.